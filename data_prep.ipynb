{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary from df for faster lookup\n",
    "postcodes_df = pd.read_csv('ukpostcodes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the rows of interest using Dask for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify file path of raw data\n",
    "file_path = 'pp-complete.csv'\n",
    "\n",
    "# Define column names\n",
    "column_names = ['transaction_id','price', 'transfer_date', 'postcode', 'property_type', 'new_build_flag', 'duration', 'primary_address', 'secondary_address', 'street', 'locality', 'city', 'district', 'county', 'ppd_category', 'record_status']\n",
    "\n",
    "# Specify data types for each column\n",
    "dtypes = dict.fromkeys(column_names, 'string')\n",
    "dtypes.update({'price': 'int64'})\n",
    "\n",
    "# Specify datetime columns for parsing\n",
    "parse_dates = ['transfer_date']\n",
    "\n",
    "# Define columns we don't want\n",
    "usecols = [col for col in column_names if col != 'transaction_id']\n",
    "\n",
    "# Read CSV using Dask with specified data types and parse_dates\n",
    "ddf = dd.read_csv(file_path, header=None, names=column_names, dtype=dtypes, parse_dates=parse_dates,usecols=usecols)\n",
    "\n",
    "# Compute to get a Pandas DataFrame\n",
    "raw_data_df = ddf.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows with no postcode, properties of type 'Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with no postcode\n",
    "clean_data_df = raw_data_df.dropna(subset=['postcode'])\n",
    "\n",
    "#Exclude property type Other (O)\n",
    "clean_data_df = clean_data_df.loc[clean_data_df['property_type']!='O']\n",
    "\n",
    "# Sort by Date\n",
    "clean_data_df.sort_values(by=['transfer_date'], inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add postcode lat and long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the postcode CSV file\n",
    "postcode_df = pd.read_csv('ukpostcodes.csv')\n",
    "\n",
    "# Convert df to dict for faster lookup\n",
    "postcodes = dict()\n",
    "for (postcode, latitude, longitude) in postcodes_df[['postcode', 'latitude', 'longitude']].values:postcodes[postcode] = [latitude, longitude]\n",
    "\n",
    "# Apply the mapping to create the new column\n",
    "clean_data_df['postcode_lat_long'] = clean_data_df['postcode'].map(postcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add postcode sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_df['postcode_sector'] =  clean_data_df['postcode'].apply(lambda x: x[:x.find(' ')+2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add region column as defined by GeoJSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load regions by postcodes\n",
    "regions_df = pd.read_csv('postcode_areas.csv', usecols=['postcode_prefix', 'region'])\n",
    "\n",
    "# Create a dictionary from the DataFrame\n",
    "postcode_region_dict = dict(zip(regions_df['postcode_prefix'], regions_df['region']))\n",
    "\n",
    "# Add postcode prefix column to main df\n",
    "clean_data_df['postcode_prefix'] = clean_data_df['postcode'].str.extract(r'^([A-Z]+)')\n",
    "\n",
    "# Map the postcode prefix to the region using the dictionary\n",
    "clean_data_df['region'] = clean_data_df['postcode_prefix'].map(postcode_region_dict)\n",
    "\n",
    "# Drop the intermediate 'postcode_prefix' column\n",
    "clean_data_df.drop(columns=['postcode_prefix'], inplace=True)\n",
    "\n",
    "#Exclude property type Other (O)\n",
    "clean_data_df = clean_data_df.loc[clean_data_df['region']!='Scotland']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add year column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year from 'transfer_date'\n",
    "clean_data_df['year'] = clean_data_df['transfer_date'].dt.year\n",
    "\n",
    "# Extract the month from 'transfer_date'\n",
    "clean_data_df['month'] = clean_data_df['transfer_date'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GeoJSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Shapefile to GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GeoJSON file for all sectors using Shapefil\n",
    "myshpfile = gpd.read_file('shapefiles/Sectors.shp')\n",
    "myshpfile.to_file('geo_json/all_sectors.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create discrete GeoJSON files for each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the postcode prefixes CSV\n",
    "postcode_prefixes_df = pd.read_csv('postcode_areas.csv')\n",
    "\n",
    "# Load the GeoJSON file containing postcode sector data\n",
    "geojson_path = 'geo_json/all_sectors.geojson'\n",
    "postcode_sectors_gdf = gpd.read_file(geojson_path)\n",
    "\n",
    "# Create an empty dictionary to store GeoDataFrames for each region\n",
    "region_geojson_dict = {}\n",
    "\n",
    "# Extract postcode prefix from 'name' property\n",
    "postcode_sectors_gdf['postcode_prefix'] = pd.Series(postcode_sectors_gdf['name']).str.extract(r'^([A-Z]+)')\n",
    "\n",
    "# Merge with postcode prefixes DataFrame to get regions\n",
    "merged_df = pd.merge(postcode_sectors_gdf, postcode_prefixes_df, how='left', on='postcode_prefix')\n",
    "\n",
    "# Create a dictionary to store GeoDataFrames for each region\n",
    "region_geojson_dict = {}\n",
    "\n",
    "# Iterate over unique regions\n",
    "for region in merged_df['region'].unique():\n",
    "    # Filter DataFrame for the current region\n",
    "    region_df = merged_df[merged_df['region'] == region].copy()\n",
    "    \n",
    "    # Create a GeoDataFrame for the current region\n",
    "    region_gdf = gpd.GeoDataFrame(region_df, geometry='geometry', crs=postcode_sectors_gdf.crs)\n",
    "    \n",
    "    # Store the GeoDataFrame in the dictionary under the region key\n",
    "    region_geojson_dict[region] = region_gdf\n",
    "\n",
    "# Now, region_geojson_dict contains GeoDataFrames for each region\n",
    "\n",
    "for region, region_gdf in region_geojson_dict.items():\n",
    "    region_geojson_path = f'geo_json/regions/{region}_postcode_sectors.geojson'\n",
    "    region_gdf.to_file(region_geojson_path, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calulate average price for each postcode by year and export processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by region, postcode_sector, and year, then calculate the rounded average price and count the number of sales\n",
    "grouped_df = clean_data_df.groupby(['region', 'postcode_sector', 'year']).agg({'price': ['mean', 'count']}).reset_index()\n",
    "grouped_df.columns = ['region', 'postcode_sector', 'year','avg_price', 'volume']\n",
    "\n",
    "# Round the average price to the nearest thousand and convert to integers\n",
    "grouped_df['avg_price'] = grouped_df['avg_price'].round(-3).astype(int)\n",
    "\n",
    "# Loop through unique years and save CSV for each year\n",
    "for year in grouped_df['year'].unique():\n",
    "    # Filter DataFrame for the current year\n",
    "    year_df = grouped_df[grouped_df['year'] == year]\n",
    "    \n",
    "    # Create the CSV file path for the current year\n",
    "    csv_file_path = (f'processed_data/average_price_by_year/region_data_{year}.csv')\n",
    "    \n",
    "    # Save the filtered DataFrame to CSV\n",
    "    year_df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 235870 entries, 0 to 235869\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   region           235870 non-null  object\n",
      " 1   postcode_sector  235870 non-null  object\n",
      " 2   year             235870 non-null  int64 \n",
      " 3   avg_price        235870 non-null  int32 \n",
      " 4   volume           235870 non-null  int64 \n",
      "dtypes: int32(1), int64(2), object(2)\n",
      "memory usage: 8.1+ MB\n"
     ]
    }
   ],
   "source": [
    "grouped_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate average price for each region and export processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year, region, property_type, and calculate rounded average price\n",
    "region_grouped_df = clean_data_df.groupby(['year', 'region', 'property_type']).agg(avg_price=('price', 'mean')).round({'avg_price': -3}).astype({'avg_price': int}).reset_index()\n",
    "\n",
    "# Calculate sales volume for each group\n",
    "region_grouped_df['volume'] = clean_data_df.groupby(['year', 'region', 'property_type']).size().reset_index(name='volume')['volume']\n",
    "\n",
    "# Save the grouped data to a single CSV file\n",
    "region_grouped_df.to_csv('processed_data/region_avg_price/region_avg_prices.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the percentage delta of average price (year on year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by region and postcode_sector, then calculate the percentage delta\n",
    "delta_df = grouped_df.copy()  # Create a new DataFrame to store the results\n",
    "delta_df['delta'] = grouped_df.groupby(['region', 'postcode_sector'])['avg_price'].pct_change() * 100\n",
    "\n",
    "# Set the percentage change for the first year to 0\n",
    "delta_df.loc[delta_df['year'] == delta_df['year'].min(), 'delta'] = 0\n",
    "\n",
    "# Drop any rows with a null delta\n",
    "delta_df = delta_df.dropna(subset=['delta'])\n",
    "\n",
    "# Round values to the nearest integer\n",
    "delta_df['delta'] = delta_df['delta'].round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of years from df\n",
    "unique_years = delta_df['year'].unique()\n",
    "\n",
    "# Iterate over each year and create a CSV file\n",
    "for year in unique_years:\n",
    "    # Filter rows for the current year\n",
    "    year_df = delta_df[delta_df['year'] == year]\n",
    "\n",
    "    # Create a CSV file for the current year\n",
    "    csv_file_path = f'processed_data/avg_price_delta/avg_price_delta_{year}.csv'\n",
    "    year_df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>transfer_date</th>\n",
       "      <th>postcode</th>\n",
       "      <th>property_type</th>\n",
       "      <th>new_build_flag</th>\n",
       "      <th>duration</th>\n",
       "      <th>primary_address</th>\n",
       "      <th>secondary_address</th>\n",
       "      <th>street</th>\n",
       "      <th>locality</th>\n",
       "      <th>city</th>\n",
       "      <th>district</th>\n",
       "      <th>county</th>\n",
       "      <th>ppd_category</th>\n",
       "      <th>record_status</th>\n",
       "      <th>postcode_lat_long</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28141180</th>\n",
       "      <td>315000</td>\n",
       "      <td>2023-09-29</td>\n",
       "      <td>LE11 3NF</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>49</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>KINGFISHER WAY</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LOUGHBOROUGH</td>\n",
       "      <td>CHARNWOOD</td>\n",
       "      <td>LEICESTERSHIRE</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>[52.765639, -1.217486]</td>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28141181</th>\n",
       "      <td>85000</td>\n",
       "      <td>2023-09-29</td>\n",
       "      <td>DL14 6SZ</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>WEST VIEW</td>\n",
       "      <td>SOUTH CHURCH</td>\n",
       "      <td>BISHOP AUCKLAND</td>\n",
       "      <td>COUNTY DURHAM</td>\n",
       "      <td>COUNTY DURHAM</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>[54.647078, -1.662091]</td>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28141182</th>\n",
       "      <td>350000</td>\n",
       "      <td>2023-09-29</td>\n",
       "      <td>HP18 0FU</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>ALMA STREET</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>AYLESBURY</td>\n",
       "      <td>BUCKINGHAMSHIRE</td>\n",
       "      <td>BUCKINGHAMSHIRE</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>[51.835956, -0.85731]</td>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28141183</th>\n",
       "      <td>465000</td>\n",
       "      <td>2023-09-29</td>\n",
       "      <td>E8 3QN</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>212A</td>\n",
       "      <td>FLAT 3</td>\n",
       "      <td>RICHMOND ROAD</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>HACKNEY</td>\n",
       "      <td>GREATER LONDON</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>[51.543252, -0.06186]</td>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28141184</th>\n",
       "      <td>125000</td>\n",
       "      <td>2023-09-29</td>\n",
       "      <td>SE14 6DQ</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>GROUND FLOOR FLAT, 59</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>CHILDERIC ROAD</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>LEWISHAM</td>\n",
       "      <td>GREATER LONDON</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>[51.477684, -0.03846]</td>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           price transfer_date  postcode property_type new_build_flag  \\\n",
       "28141180  315000    2023-09-29  LE11 3NF             T              N   \n",
       "28141181   85000    2023-09-29  DL14 6SZ             T              N   \n",
       "28141182  350000    2023-09-29  HP18 0FU             T              N   \n",
       "28141183  465000    2023-09-29    E8 3QN             F              N   \n",
       "28141184  125000    2023-09-29  SE14 6DQ             F              N   \n",
       "\n",
       "         duration        primary_address secondary_address          street  \\\n",
       "28141180        F                     49              <NA>  KINGFISHER WAY   \n",
       "28141181        F                      8              <NA>       WEST VIEW   \n",
       "28141182        F                      5              <NA>     ALMA STREET   \n",
       "28141183        L                   212A            FLAT 3   RICHMOND ROAD   \n",
       "28141184        L  GROUND FLOOR FLAT, 59              <NA>  CHILDERIC ROAD   \n",
       "\n",
       "              locality             city         district           county  \\\n",
       "28141180          <NA>     LOUGHBOROUGH        CHARNWOOD   LEICESTERSHIRE   \n",
       "28141181  SOUTH CHURCH  BISHOP AUCKLAND    COUNTY DURHAM    COUNTY DURHAM   \n",
       "28141182          <NA>        AYLESBURY  BUCKINGHAMSHIRE  BUCKINGHAMSHIRE   \n",
       "28141183          <NA>           LONDON          HACKNEY   GREATER LONDON   \n",
       "28141184          <NA>           LONDON         LEWISHAM   GREATER LONDON   \n",
       "\n",
       "         ppd_category record_status       postcode_lat_long  year  month  \n",
       "28141180            B             A  [52.765639, -1.217486]  2023      9  \n",
       "28141181            A             A  [54.647078, -1.662091]  2023      9  \n",
       "28141182            B             A   [51.835956, -0.85731]  2023      9  \n",
       "28141183            A             A   [51.543252, -0.06186]  2023      9  \n",
       "28141184            A             A   [51.477684, -0.03846]  2023      9  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create volume by month for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by region, postcode_sector, year, and month, then calculate the count of sales\n",
    "volume_df = clean_data_df.groupby(['region', 'postcode_sector', 'year', 'month']).agg({'price': 'count'}).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "volume_df.columns = ['region', 'postcode_sector', 'year', 'month', 'volume']\n",
    "\n",
    "# Loop through unique years\n",
    "for year in volume_df['year'].unique():\n",
    "    # Filter DataFrame for the current year\n",
    "    year_df = volume_df[volume_df['year'] == year]\n",
    "    \n",
    "    # Group by region and month, then calculate the total volume\n",
    "    total_volume_df = year_df.groupby(['region', 'month']).agg({'volume': 'sum'}).reset_index()\n",
    "    \n",
    "    # Create the CSV file path for the current year and save the DataFrame\n",
    "    csv_file_path = f'processed_data/volume_by_year/region_total_volume_{year}.csv'\n",
    "    total_volume_df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
